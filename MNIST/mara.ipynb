{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a1aa40",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "## Imports\n",
    "Install and import project dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcf19cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install matplotlib numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd42c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils import clip_grad_norm_  \n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from snntorch import surrogate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import struct\n",
    "from os.path import join\n",
    "from typing import Tuple\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa05d52c",
   "metadata": {},
   "source": [
    "## MNIST Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d2d0e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC_LABELS = 2049\n",
    "MAGIC_IMAGES = 2051\n",
    "\n",
    "class MnistDataloader:\n",
    "    def __init__(self, train_images, train_labels, test_images, test_labels):\n",
    "        self.train_images = train_images\n",
    "        self.train_labels = train_labels\n",
    "        self.test_images = test_images\n",
    "        self.test_labels = test_labels\n",
    "        \n",
    "    def _read_idx_labels(self, path: str) -> np.ndarray:\n",
    "        \"\"\"Return labels in an `ndarray` of shape `(n,).`\"\"\"\n",
    "        with open(path, \"rb\") as f:\n",
    "            magic, n = struct.unpack(\">II\", f.read(8))\n",
    "            if magic != MAGIC_LABELS:\n",
    "                raise ValueError(f\"Labels magic mismatch: expected {MAGIC_LABELS}, got {magic}\")\n",
    "            \n",
    "            labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "            if labels.size != n:\n",
    "                raise ValueError(f\"Expected {n} labels, found {labels.size}\")\n",
    "            \n",
    "            return labels\n",
    "\n",
    "    def _read_idx_images(self, path: str) -> np.ndarray:\n",
    "        \"\"\"Return images in an `ndarray` of shape `(n, 28, 28)`.\"\"\"\n",
    "        with open(path, \"rb\") as f:\n",
    "            magic, n, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "            if magic != MAGIC_IMAGES:\n",
    "                raise ValueError(f\"Images magic mismatch: expected {MAGIC_IMAGES}, got {magic}\")\n",
    "            \n",
    "            pixels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "            expected = n * rows * cols\n",
    "            if pixels.size != expected:\n",
    "                raise ValueError(f\"Expected {expected} pixels, found {pixels.size}\")\n",
    "            \n",
    "            return pixels.reshape(n, rows, cols)\n",
    "\n",
    "    def load_data(\n",
    "        self,\n",
    "        normalize: bool = False,\n",
    "        dtype: np.dtype = np.float32\n",
    "    ) -> Tuple[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"Return tuples of the train and test data.\"\"\"\n",
    "        X_train = self._read_idx_images(self.train_images)\n",
    "        y_train = self._read_idx_labels(self.train_labels)\n",
    "        X_test  = self._read_idx_images(self.test_images)\n",
    "        y_test  = self._read_idx_labels(self.test_labels)\n",
    "\n",
    "        if normalize:\n",
    "            X_train = (X_train.astype(dtype) / 255.0)\n",
    "            X_test  = (X_test.astype(dtype) / 255.0)\n",
    "\n",
    "        return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c80d9a9",
   "metadata": {},
   "source": [
    "---\n",
    "# Data\n",
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a67cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"./Data\"\n",
    "training_images_filepath = join(input_path, \"train-images-idx3-ubyte/train-images-idx3-ubyte\")\n",
    "training_labels_filepath = join(input_path, \"train-labels-idx1-ubyte/train-labels-idx1-ubyte\")\n",
    "test_images_filepath = join(input_path, \"t10k-images-idx3-ubyte/t10k-images-idx3-ubyte\")\n",
    "test_labels_filepath = join(input_path, \"t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80b27862",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(X_train, y_train), (X_test, y_test) = mnist_dataloader.load_data(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16500ea4",
   "metadata": {},
   "source": [
    "## Prepare data for PyTorch\n",
    "\n",
    "Convert NumPy to PyTorch tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2371a026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensors and flatten the images to 784-d vectors (instead of 28x28)\n",
    "Xtr = torch.tensor(X_train, dtype=torch.float32).view(-1, 28*28)\n",
    "Ytr = torch.tensor(y_train, dtype=torch.long)\n",
    "Xte = torch.tensor(X_test, dtype=torch.float32).view(-1, 28*28)\n",
    "Yte = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf05725f",
   "metadata": {},
   "source": [
    "Build DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00a053c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle = true ensures random batches each epoch\n",
    "train_loader = DataLoader(TensorDataset(Xtr, Ytr), batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(TensorDataset(Xte, Yte), batch_size=128, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e8b6f8",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d32152e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "input_size = 28*28\n",
    "output_size = 10\n",
    "hidden_size = 1000\n",
    "T = 50\n",
    "beta = 0.95 # membrane decay constant\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spike_grad = surrogate.fast_sigmoid(slope=25) # surrogate d/dV for backprop\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # two layer fully connected network with LIF neurons\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.lif1 = snn.Leaky(beta = beta, spike_grad=spike_grad)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.lif2 = snn.Leaky(beta = beta, spike_grad=spike_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        \n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "        \n",
    "        # repeat the same image for T steps (no encoding)\n",
    "        for step in range(T):\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0) # return (spikes, membrane) over time with shape [T,B,10]\n",
    "\n",
    "# move model to device\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e992962a",
   "metadata": {},
   "source": [
    "# Training of the SNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bcd66b",
   "metadata": {},
   "source": [
    "## Defining the entropy loss function and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e0c0ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d53d5",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7df077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(): \n",
    "    \n",
    "    # training for one epoch\n",
    "    \n",
    "    net.train()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    \n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        \n",
    "        # forward pass through time\n",
    "        spk_rec, mem_rec = net(xb)\n",
    "        logits = mem_rec.sum(0)\n",
    "        \n",
    "        \n",
    "        loss = criterion(logits, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(net.parameters(), max_norm=1.0)  # gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * yb.size(0)\n",
    "        total_correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total_samples += yb.size(0)\n",
    "        \n",
    "    return total_loss / total_samples, total_correct / total_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54590841",
   "metadata": {},
   "source": [
    "## Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1275aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    net.eval()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    \n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        \n",
    "        spk_rec, mem_rec = net(xb)\n",
    "        logits = mem_rec.sum(0)\n",
    "        \n",
    "        loss = criterion(logits, yb)\n",
    "        \n",
    "        total_loss += loss.item() * yb.size(0)\n",
    "        total_correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total_samples += yb.size(0)\n",
    "\n",
    "    return total_loss / total_samples, total_correct / total_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e80d21",
   "metadata": {},
   "source": [
    "## Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8514db44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train: loss=0.8240, acc=0.917 | Test:  loss=0.2476,  acc=0.958 | Best: 0.958\n",
      "Epoch 02 | Train: loss=0.2003, acc=0.964 | Test:  loss=0.2234,  acc=0.970 | Best: 0.970\n",
      "Epoch 03 | Train: loss=0.1433, acc=0.975 | Test:  loss=0.2586,  acc=0.966 | Best: 0.970\n",
      "Epoch 04 | Train: loss=0.0986, acc=0.981 | Test:  loss=0.2421,  acc=0.973 | Best: 0.973\n",
      "Epoch 05 | Train: loss=0.0789, acc=0.985 | Test:  loss=0.2901,  acc=0.975 | Best: 0.975\n",
      "Training complete. Best Test Accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "best_acc = 0.0\n",
    "\n",
    "# training history\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch()\n",
    "    test_loss, test_acc = evaluate()\n",
    "    \n",
    "    # save metrics for later plots\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"test_loss\"].append(test_loss)\n",
    "    history[\"test_acc\"].append(test_acc)\n",
    "    \n",
    "    # save best model\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(net.state_dict(), \"best_mnist_snn.pth\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"Train: loss={train_loss:.4f}, acc={train_acc:.3f} | \"\n",
    "        f\"Test:  loss={test_loss:.4f},  acc={test_acc:.3f} | \"\n",
    "        f\"Best: {best_acc:.3f}\")\n",
    "    \n",
    "print(\"Training complete. Best Test Accuracy: {:.3f}\".format(best_acc))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
