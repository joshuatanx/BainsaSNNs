{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tonic\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision\n",
    "from snntorch import functional as SF\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/matteogiardina/Desktop/BEMACS 2/SNNs/BainsaSNNs/N-MNIST/n-mnist_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tonic.transforms as transforms\n",
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "\n",
    "frame_transform = transforms.Compose(\n",
    "    [transforms.Denoise(filter_time = 10000),\n",
    "     transforms.ToFrame(sensor_size = sensor_size, time_window = 1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = tonic.datasets.NMNIST(save_to=root, transform = frame_transform, train=True)\n",
    "testset = tonic.datasets.NMNIST(save_to=root, transform = frame_transform,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tonic import DiskCachedDataset\n",
    "\n",
    "cached_trainset = DiskCachedDataset(trainset, cache_path = \"./cache/nmnist/train\")\n",
    "cached_dataloader = DataLoader(cached_trainset)\n",
    "\n",
    "batch_size = 128\n",
    "trainloader = DataLoader(cached_trainset,\n",
    "                         batch_size = batch_size,\n",
    "                         collate_fn = tonic.collation.PadTensors)\n",
    "\n",
    "def load_sample_batched():\n",
    "    events, target = next(iter(cached_dataloader))\n",
    "\n",
    "load_sample_batched()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tonic.transforms.Compose([torch.from_numpy,\n",
    "                                      torchvision.transforms.RandomRotation([-10,10])])\n",
    "\n",
    "cached_trainset = DiskCachedDataset(trainset, transform=transform, cache_path = \"./cache/mnist/train\")\n",
    "# No augmentation (transform) for the test set\n",
    "cached_testset = DiskCachedDataset(testset, cache_path = \"./cache/mnist/train\")\n",
    "\n",
    "batch_size = 128\n",
    "trainloader = DataLoader(cached_trainset, batch_size = batch_size,\n",
    "                         collate_fn = tonic.collation.PadTensors(batch_first = False), shuffle = True)\n",
    "testloader = DataLoader(cached_testset, batch_size = batch_size,\n",
    "                        collate_fn = tonic.collation.PadTensors(batch_first = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synaptic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Hyperparameters ---\n",
    "# spike_grad = surrogate.atan()\n",
    "# # We need TWO decay rates now: one for current (alpha) and one for membrane (beta)\n",
    "# # Let's start with some common defaults. You can tune these.\n",
    "# alpha = 0.9  # Synaptic current decay\n",
    "# beta = 0.95   # Membrane potential decay\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# # --- Define the Synaptic CNN Class ---\n",
    "# class SynapticCNN(nn.Module):\n",
    "#     def __init__(self, alpha, beta, spike_grad):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # --- Layer 1 ---\n",
    "#         self.conv1 = nn.Conv2d(2, 12, 5)\n",
    "#         self.pool1 = nn.MaxPool2d(2)\n",
    "#         self.lif1 = snn.Synaptic(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
    "        \n",
    "#         # --- Layer 2 ---\n",
    "#         self.conv2 = nn.Conv2d(12, 32, 5)\n",
    "#         self.pool2 = nn.MaxPool2d(2)\n",
    "#         self.lif2 = snn.Synaptic(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
    "        \n",
    "#         # --- Output Layer ---\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.fc1 = nn.Linear(32*5*5, 10)\n",
    "#         self.lif_out = snn.Synaptic(alpha=alpha, beta=beta, spike_grad=spike_grad, output=True)\n",
    "\n",
    "#     def forward(self, x_seq):\n",
    "#         # x_seq has shape [T, Batch, C, H, W]\n",
    "\n",
    "#         # --- Initialize Hidden States ---\n",
    "#         # We must do this manually outside the loop\n",
    "#         syn1, mem1 = self.lif1.init_synaptic()\n",
    "#         syn2, mem2 = self.lif2.init_synaptic()\n",
    "#         syn_out, mem_out = self.lif_out.init_synaptic()\n",
    "        \n",
    "#         # This will record the output spikes at each step\n",
    "#         spk_rec = []\n",
    "#         mem_rec = []\n",
    "        \n",
    "#         # --- Temporal Loop (now inside the model) ---\n",
    "#         for step in range(x_seq.size(0)):\n",
    "#             x_step = x_seq[step] # Get input at current time step\n",
    "            \n",
    "#             # 1. Pass through Layer 1\n",
    "#             cur1 = self.pool1(self.conv1(x_step))\n",
    "#             spk1, syn1, mem1 = self.lif1(cur1, syn1, mem1) # Pass states back in\n",
    "\n",
    "#             # 2. Pass through Layer 2\n",
    "#             cur2 = self.pool2(self.conv2(spk1))\n",
    "#             spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
    "\n",
    "#             # 3. Pass through Output Layer\n",
    "#             flat = self.flatten(spk2)\n",
    "#             cur_out = self.fc1(flat)\n",
    "#             spk_out, syn_out, mem_out = self.lif_out(cur_out, syn_out, mem_out)\n",
    "            \n",
    "#             spk_rec.append(spk_out)\n",
    "#             mem_rec.append(mem_out)\n",
    "\n",
    "#         # Return all output spikes [T, Batch, 10]\n",
    "#         return torch.stack(spk_rec, dim=0), torch.stack(mem_rec, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters ---\n",
    "spike_grad = surrogate.atan()\n",
    "beta = 0.7   # Membrane potential decay (good, keep this)\n",
    "threshold = 0.2 # ** NEW: Lowered threshold to encourage spiking **\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# --- Define the Leaky CNN Class ---\n",
    "class LeakyCNN(nn.Module):\n",
    "    def __init__(self, beta, spike_grad, threshold):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Layer 1 ---\n",
    "        self.conv1 = nn.Conv2d(2, 12, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad, threshold=threshold)\n",
    "        \n",
    "        # --- Layer 2 ---\n",
    "        self.conv2 = nn.Conv2d(12, 32, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad, threshold=threshold)\n",
    "        \n",
    "        # --- Output Layer ---\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32*5*5, 10)\n",
    "        self.lif_out = snn.Leaky(beta=beta, spike_grad=spike_grad, threshold=threshold, output=True)\n",
    "\n",
    "    def forward(self, x_seq):\n",
    "        # x_seq has shape [T, Batch, C, H, W]\n",
    "\n",
    "        # --- Initialize Hidden States ---\n",
    "        # ** CHANGED: init_leaky() only returns mem **\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem_out = self.lif_out.init_leaky()\n",
    "        \n",
    "        spk_rec = []\n",
    "        mem_rec = []\n",
    "        \n",
    "        # --- Temporal Loop ---\n",
    "        for step in range(x_seq.size(0)):\n",
    "            x_step = x_seq[step]\n",
    "            \n",
    "            # 1. Pass through Layer 1\n",
    "            cur1 = self.pool1(self.conv1(x_step))\n",
    "            spk1, mem1 = self.lif1(cur1, mem1) \n",
    "\n",
    "            # 2. Pass through Layer 2\n",
    "            cur2 = self.pool2(self.conv2(spk1))\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            # 3. Pass through Output Layer\n",
    "            flat = self.flatten(spk2)\n",
    "            cur_out = self.fc1(flat)\n",
    "            \n",
    "            spk_out, mem_out = self.lif_out(cur_out, mem_out)\n",
    "            \n",
    "            spk_rec.append(spk_out)\n",
    "            mem_rec.append(mem_out) \n",
    "\n",
    "        return torch.stack(spk_rec, dim=0), torch.stack(mem_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = SynapticCNN(alpha = alpha, beta = beta, spike_grad = spike_grad).to(device)\n",
    "\n",
    "net = LeakyCNN(beta=beta, spike_grad=spike_grad, threshold=threshold).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr = 2e-3, betas = (0.9, 0.999))\n",
    "loss_fn = SF.mse_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0 \n",
      "Train Loss: 448.58\n",
      "Accuracy: 17.19%\n",
      "\n",
      "Epoch 0, Iteration 1 \n",
      "Train Loss: 554.55\n",
      "Accuracy: 7.03%\n",
      "\n",
      "Epoch 0, Iteration 2 \n",
      "Train Loss: 517.61\n",
      "Accuracy: 9.38%\n",
      "\n",
      "Epoch 0, Iteration 3 \n",
      "Train Loss: 535.44\n",
      "Accuracy: 8.59%\n",
      "\n",
      "Epoch 0, Iteration 4 \n",
      "Train Loss: 454.49\n",
      "Accuracy: 8.59%\n",
      "\n",
      "Epoch 0, Iteration 5 \n",
      "Train Loss: 468.26\n",
      "Accuracy: 12.50%\n",
      "\n",
      "Epoch 0, Iteration 6 \n",
      "Train Loss: 493.25\n",
      "Accuracy: 4.69%\n",
      "\n",
      "Epoch 0, Iteration 7 \n",
      "Train Loss: 465.36\n",
      "Accuracy: 10.94%\n",
      "\n",
      "Epoch 0, Iteration 8 \n",
      "Train Loss: 537.54\n",
      "Accuracy: 6.25%\n",
      "\n",
      "Epoch 0, Iteration 9 \n",
      "Train Loss: 550.62\n",
      "Accuracy: 4.69%\n",
      "\n",
      "Epoch 0, Iteration 10 \n",
      "Train Loss: 556.05\n",
      "Accuracy: 8.59%\n",
      "\n",
      "Epoch 0, Iteration 11 \n",
      "Train Loss: 555.95\n",
      "Accuracy: 8.59%\n",
      "\n",
      "Epoch 0, Iteration 12 \n",
      "Train Loss: 537.21\n",
      "Accuracy: 10.16%\n",
      "\n",
      "Epoch 0, Iteration 13 \n",
      "Train Loss: 449.08\n",
      "Accuracy: 8.59%\n",
      "\n",
      "Epoch 0, Iteration 14 \n",
      "Train Loss: 506.67\n",
      "Accuracy: 10.16%\n",
      "\n",
      "Epoch 0, Iteration 15 \n",
      "Train Loss: 549.89\n",
      "Accuracy: 10.16%\n",
      "\n",
      "Epoch 0, Iteration 16 \n",
      "Train Loss: 490.61\n",
      "Accuracy: 12.50%\n",
      "\n",
      "Epoch 0, Iteration 17 \n",
      "Train Loss: 402.58\n",
      "Accuracy: 11.72%\n",
      "\n",
      "Epoch 0, Iteration 18 \n",
      "Train Loss: 451.26\n",
      "Accuracy: 8.59%\n",
      "\n",
      "Epoch 0, Iteration 19 \n",
      "Train Loss: 590.94\n",
      "Accuracy: 7.81%\n",
      "\n",
      "Epoch 0, Iteration 20 \n",
      "Train Loss: 495.45\n",
      "Accuracy: 8.59%\n",
      "\n",
      "Epoch 0, Iteration 21 \n",
      "Train Loss: 475.60\n",
      "Accuracy: 13.28%\n",
      "\n",
      "Epoch 0, Iteration 22 \n",
      "Train Loss: 461.86\n",
      "Accuracy: 7.81%\n",
      "\n",
      "Epoch 0, Iteration 23 \n",
      "Train Loss: 545.80\n",
      "Accuracy: 11.72%\n",
      "\n",
      "Epoch 0, Iteration 24 \n",
      "Train Loss: 476.26\n",
      "Accuracy: 11.72%\n",
      "\n",
      "Epoch 0, Iteration 25 \n",
      "Train Loss: 433.03\n",
      "Accuracy: 10.16%\n",
      "\n",
      "Epoch 0, Iteration 26 \n",
      "Train Loss: 558.37\n",
      "Accuracy: 9.38%\n",
      "\n",
      "Epoch 0, Iteration 27 \n",
      "Train Loss: 611.93\n",
      "Accuracy: 13.28%\n",
      "\n",
      "Epoch 0, Iteration 28 \n",
      "Train Loss: 539.46\n",
      "Accuracy: 14.84%\n",
      "\n",
      "Epoch 0, Iteration 29 \n",
      "Train Loss: 564.40\n",
      "Accuracy: 11.72%\n",
      "\n",
      "Epoch 0, Iteration 30 \n",
      "Train Loss: 466.45\n",
      "Accuracy: 8.59%\n",
      "\n",
      "Epoch 0, Iteration 31 \n",
      "Train Loss: 546.12\n",
      "Accuracy: 7.81%\n",
      "\n",
      "Epoch 0, Iteration 32 \n",
      "Train Loss: 546.71\n",
      "Accuracy: 3.12%\n",
      "\n",
      "Epoch 0, Iteration 33 \n",
      "Train Loss: 573.58\n",
      "Accuracy: 4.69%\n",
      "\n",
      "Epoch 0, Iteration 34 \n",
      "Train Loss: 591.59\n",
      "Accuracy: 8.59%\n",
      "\n",
      "Epoch 0, Iteration 35 \n",
      "Train Loss: 569.12\n",
      "Accuracy: 6.25%\n",
      "\n",
      "Epoch 0, Iteration 36 \n",
      "Train Loss: 500.66\n",
      "Accuracy: 7.81%\n",
      "\n",
      "Epoch 0, Iteration 37 \n",
      "Train Loss: 503.82\n",
      "Accuracy: 9.38%\n",
      "\n",
      "Epoch 0, Iteration 38 \n",
      "Train Loss: 391.25\n",
      "Accuracy: 10.94%\n",
      "\n",
      "Epoch 0, Iteration 39 \n",
      "Train Loss: 541.42\n",
      "Accuracy: 7.81%\n",
      "\n",
      "Epoch 0, Iteration 40 \n",
      "Train Loss: 512.06\n",
      "Accuracy: 11.72%\n",
      "\n",
      "Epoch 0, Iteration 41 \n",
      "Train Loss: 465.64\n",
      "Accuracy: 10.94%\n",
      "\n",
      "Epoch 0, Iteration 42 \n",
      "Train Loss: 520.91\n",
      "Accuracy: 11.72%\n",
      "\n",
      "Epoch 0, Iteration 43 \n",
      "Train Loss: 521.70\n",
      "Accuracy: 10.16%\n",
      "\n",
      "Epoch 0, Iteration 44 \n",
      "Train Loss: 520.09\n",
      "Accuracy: 10.16%\n",
      "\n",
      "Epoch 0, Iteration 45 \n",
      "Train Loss: 563.15\n",
      "Accuracy: 7.81%\n",
      "\n",
      "Epoch 0, Iteration 46 \n",
      "Train Loss: 516.66\n",
      "Accuracy: 11.72%\n",
      "\n",
      "Epoch 0, Iteration 47 \n",
      "Train Loss: 551.59\n",
      "Accuracy: 10.94%\n",
      "\n",
      "Epoch 0, Iteration 48 \n",
      "Train Loss: 419.40\n",
      "Accuracy: 12.50%\n",
      "\n",
      "Epoch 0, Iteration 49 \n",
      "Train Loss: 496.27\n",
      "Accuracy: 9.38%\n",
      "\n",
      "Epoch 0, Iteration 50 \n",
      "Train Loss: 589.65\n",
      "Accuracy: 10.94%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1 # never reaching it\n",
    "num_iters = 50\n",
    "\n",
    "loss_hist = []\n",
    "acc_hist = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, targets) in enumerate(iter(trainloader)):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        net.train()\n",
    "\n",
    "        spk_rec, mem_rec = net(data)\n",
    "        \n",
    "        # mem_sum = torch.sum(mem_rec, dim = 0)\n",
    "        loss_val = loss_fn(mem_sum, targets)\n",
    "\n",
    "        # Gradient calculation + weights update\n",
    "        optimizer.zero_grad() # resets the gradients of the previous iteration\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store the loss in the history for plotting purposes\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        print(f\"Epoch {epoch}, Iteration {i} \\nTrain Loss: {loss_val.item():.2f}\")\n",
    "        \n",
    "        acc = SF.accuracy_rate(spk_rec, targets)\n",
    "\n",
    "        acc_hist.append(acc)\n",
    "        print(f\"Accuracy: {acc*100:.2f}%\\n\")\n",
    "\n",
    "        # Training loop breaks after 50 iterations\n",
    "        if i == num_iters:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
